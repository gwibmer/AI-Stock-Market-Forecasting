{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, LeakyReLU\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = '1960-01-01'\n",
    "TRAIN_END_DATE = '2015-12-31'\n",
    "PREDICT_START_DATE = '2016-01-01'\n",
    "PREDICT_END_DATE = '2019-12-31'\n",
    "WINDOW_SIZE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download S&P 500 data from Yahoo Finance\n",
    "df = yf.download('^GSPC', start=TRAIN_START_DATE, end=PREDICT_END_DATE)\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "def str_to_datetime(s):\n",
    "  split = s.split('-')\n",
    "  year, month, day = int(split[0]), int(split[1]), int(split[2])\n",
    "  return datetime.datetime(year=year, month=month, day=day)\n",
    "\n",
    "df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
    "df['Date'] = df['Date'].apply(str_to_datetime)\n",
    "\n",
    "df.index = df.pop('Date')\n",
    "\n",
    "# Drop columns that are not needed\n",
    "df = df.drop(columns=['Adj Close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train = df[:PREDICT_START_DATE]\n",
    "X_test = df[PREDICT_START_DATE:PREDICT_END_DATE]\n",
    "\n",
    "y_train = X_train.pop('Close')\n",
    "y_test = X_test.pop('Close')\n",
    "\n",
    "def df_to_windowed_df(feature, df, window_size, df_slice=slice(None, None)):\n",
    "    \"\"\"Converts a dataframe into a windowed dataframe and date list\"\"\"\n",
    "    df = df.reindex(df_slice)  # Reindex the dataframe to include missing dates\n",
    "    df = df.dropna()  # Drop rows with NaN values\n",
    "    date_list = df.index.to_list()\n",
    "    feature_values = df[feature].to_numpy()\n",
    "\n",
    "    windowed_df = []\n",
    "    for i in range(len(feature_values) - window_size + 1):\n",
    "        windowed_df.append(feature_values[i:i + window_size])\n",
    "\n",
    "    return np.array(windowed_df), date_list[window_size:]\n",
    "\n",
    "\n",
    "# Create windowed dataframes and date_list\n",
    "open_windowed_df, date_list = df_to_windowed_df('Open', df, window_size=WINDOW_SIZE, df_slice=pd.date_range(PREDICT_START_DATE, PREDICT_END_DATE))\n",
    "high_windowed_df, date_list = df_to_windowed_df('High', df, window_size=WINDOW_SIZE, df_slice=pd.date_range(PREDICT_START_DATE, PREDICT_END_DATE))\n",
    "low_windowed_df, date_list = df_to_windowed_df('Low', df, window_size=WINDOW_SIZE, df_slice=pd.date_range(PREDICT_START_DATE, PREDICT_END_DATE))\n",
    "volume_windowed_df, date_list = df_to_windowed_df('Volume', df, window_size=WINDOW_SIZE, df_slice=pd.date_range(PREDICT_START_DATE, PREDICT_END_DATE))\n",
    "\n",
    "to_combine_open_windowed_df, _ = df_to_windowed_df('Open', df, window_size=1, df_slice=pd.date_range(PREDICT_START_DATE, PREDICT_END_DATE))\n",
    "to_combine_high_windowed_df, _ = df_to_windowed_df('High', df, window_size=1, df_slice=pd.date_range(PREDICT_START_DATE, PREDICT_END_DATE))\n",
    "to_combine_low_windowed_df, _ = df_to_windowed_df('Low', df, window_size=1, df_slice=pd.date_range(PREDICT_START_DATE, PREDICT_END_DATE))\n",
    "to_combine_volume_windowed_df, _ = df_to_windowed_df('Volume', df, window_size=1, df_slice=pd.date_range(PREDICT_START_DATE, PREDICT_END_DATE))\n",
    "\n",
    "# Stack the windowed dataframes along the third axis\n",
    "stacked_windowed_df = np.stack([open_windowed_df, high_windowed_df, low_windowed_df, volume_windowed_df], axis=-1)\n",
    "\n",
    "# Reshape the stacked_windowed_df into the desired shape (975, 30, 4)\n",
    "combined_windowed_df = stacked_windowed_df.reshape((-1, WINDOW_SIZE, 4))\n",
    "\n",
    "def windowed_df_to_date_X_y(windowed_df, date_list):\n",
    "    \"\"\"Converts a windowed dataframe and date list into a date, X, and y dataframe\"\"\"\n",
    "    date_df = []\n",
    "    X_df = []\n",
    "    y_df = []\n",
    "    for i in range(len(windowed_df) - 1):\n",
    "        if i + 1 < len(date_list):  # Add this condition to prevent IndexError\n",
    "            date_df.append(date_list[i + 1])   # Shift date by 1\n",
    "            X_df.append(windowed_df[i])\n",
    "            if windowed_df.ndim == 3:\n",
    "                y_df.append(windowed_df[i + 1, -1, 0])  # Modify the index\n",
    "            else:\n",
    "                y_df.append(windowed_df[i + 1, -1])\n",
    "    return np.array(date_df), np.array(X_df), np.array(y_df)\n",
    "\n",
    "# Use the modified function with the date_list parameter\n",
    "date_df, X_open, y_open = windowed_df_to_date_X_y(open_windowed_df, date_list)\n",
    "_, X_high, y_high = windowed_df_to_date_X_y(high_windowed_df, date_list)\n",
    "_, X_low, y_low = windowed_df_to_date_X_y(low_windowed_df, date_list)\n",
    "_, X_volume, y_volume = windowed_df_to_date_X_y(volume_windowed_df, date_list)\n",
    "_, X_train_combined, _ = windowed_df_to_date_X_y(combined_windowed_df, date_list)\n",
    "y_train_combined = y_train[len(y_train) - len(X_train_combined):].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists for X and y\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Iterate through the dataframe with a step of 1, leaving out the last WINDOW_SIZE rows\n",
    "for i in range(len(df) - WINDOW_SIZE):\n",
    "    # Get the data for the current window\n",
    "    window_data = df.iloc[i:i + WINDOW_SIZE]\n",
    "\n",
    "    # Extract the relevant features and append to X as a 2D array\n",
    "    X.append(window_data[['Open', 'High', 'Low', 'Volume']].values)\n",
    "\n",
    "    # Extract the target (close price) for the day after the window and append to y\n",
    "    y.append(df.iloc[i + WINDOW_SIZE]['Close'])\n",
    "\n",
    "# Convert X and y to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices that correspond to the start and end dates\n",
    "train_start_index = df.index.asof(pd.to_datetime(TRAIN_START_DATE))\n",
    "train_end_index = df.index.asof(pd.to_datetime(TRAIN_END_DATE))\n",
    "predict_start_index = df.index.asof(pd.to_datetime(PREDICT_START_DATE))\n",
    "predict_end_index = df.index.asof(pd.to_datetime(PREDICT_END_DATE))\n",
    "\n",
    "# Handle the case where asof() returns NaT\n",
    "train_start_index = df.index[0] if pd.isna(train_start_index) else train_start_index\n",
    "\n",
    "# Get the integer locations for the indices\n",
    "train_start_index = df.index.get_loc(train_start_index)\n",
    "train_end_index = df.index.get_loc(train_end_index)\n",
    "predict_start_index = df.index.get_loc(predict_start_index)\n",
    "predict_end_index = df.index.get_loc(predict_end_index)\n",
    "\n",
    "# Adjust the indices to account for the window size\n",
    "train_start_index = max(0, train_start_index - WINDOW_SIZE)\n",
    "train_end_index -= WINDOW_SIZE\n",
    "predict_start_index -= WINDOW_SIZE\n",
    "predict_end_index -= WINDOW_SIZE\n",
    "\n",
    "predict_end_index = predict_start_index + 997\n",
    "\n",
    "# Slice the X and y arrays using the indices\n",
    "X_train = X[train_start_index:train_end_index]\n",
    "y_train = y[train_start_index:train_end_index]\n",
    "X_test = X[predict_start_index:predict_end_index]\n",
    "y_test = y[predict_start_index:predict_end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the X_train and y_train datasets into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the input data\n",
    "mean = X_train.mean(axis=(0, 1))\n",
    "std = X_train.std(axis=(0, 1))\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_val = (X_val - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that creates the model with desired parameters\n",
    "def create_model(lstm_units=50, activation='relu', optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(lstm_units, activation=activation, input_shape=(WINDOW_SIZE, 4)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "# Create an instance of KerasRegressor\n",
    "estimator = KerasRegressor(build_fn=create_model, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Set up the parameter grid\n",
    "param_grid = {\n",
    "    'lstm_units': [50, 100, 150],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'optimizer': ['adam', 'rmsprop']\n",
    "}\n",
    "\n",
    "# Create an instance of GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=3)\n",
    "\n",
    "# Train the model using grid search\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found: \", grid_result.best_params_)\n",
    "\n",
    "# Train the model using the best parameters found\n",
    "best_model = create_model(\n",
    "    lstm_units=grid_result.best_params_['lstm_units'],\n",
    "    activation=grid_result.best_params_['activation'],\n",
    "    optimizer=grid_result.best_params_['optimizer']\n",
    ")\n",
    "\n",
    "best_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    verbose=1,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the X_test data\n",
    "X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "# Predict using the X_test dataset\n",
    "y_pred = best_model.predict(X_test_normalized)\n",
    "\n",
    "# Plot the predicted values and the actual values from y_test\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(date_df, y_test, label='Actual')\n",
    "plt.plot(date_df, y_pred, label='Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.title('S&P 500 Close Price Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_open = tf.keras.models.load_model('open_model.h5')\n",
    "model2_high = tf.keras.models.load_model('high_model.h5')\n",
    "model2_low = tf.keras.models.load_model('low_model.h5')\n",
    "model2_volume = tf.keras.models.load_model('volume_model.h5')\n",
    "\n",
    "# Predict using model 2\n",
    "y_pred_open = model2_open.predict(X_open)\n",
    "y_pred_high = model2_high.predict(X_high)\n",
    "y_pred_low = model2_low.predict(X_low)\n",
    "y_pred_volume = model2_volume.predict(X_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(data, window_size):\n",
    "    windows = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        windows.append(data[i:i + window_size])\n",
    "    return np.array(windows)\n",
    "\n",
    "# Assuming y_pred_open, y_pred_high, y_pred_low, and y_pred_volume are already defined\n",
    "# Create windows for each feature\n",
    "open_windows = create_windows(y_pred_open, WINDOW_SIZE)\n",
    "high_windows = create_windows(y_pred_high, WINDOW_SIZE)\n",
    "low_windows = create_windows(y_pred_low, WINDOW_SIZE)\n",
    "volume_windows = create_windows(y_pred_volume, WINDOW_SIZE)\n",
    "\n",
    "# Combine the 4 arrays along axis 2\n",
    "X_test2 = np.stack([open_windows, high_windows, low_windows, volume_windows], axis=2)\n",
    "X_test2 = np.squeeze(X_test2, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the X_test data\n",
    "X_test2_normalized = (X_test2 - mean) / std\n",
    "\n",
    "# Predict using the X_test dataset\n",
    "model2_pred = best_model.predict(X_test2_normalized)\n",
    "\n",
    "# Plot the predicted values and the actual values from y_test\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(date_df, y_test, label='Actual')\n",
    "plt.plot(date_df[7:], model2_pred, label='Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.title('S&P 500 Close Price Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted values and the actual values from y_test\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(date_df, y_test, label='Close')\n",
    "plt.plot(date_df[7:], model2_pred, label='Close`')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.title('S&P 500 Close Price Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "# Evaluate model performance\n",
    "mse = sklearn.metrics.mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean squared error: \", mse)\n",
    "\n",
    "mae = sklearn.metrics.mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean absolute error: \", mae)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
