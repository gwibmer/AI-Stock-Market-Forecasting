{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stock-Market-Index-Price-Prediction\n",
        "\n",
        "### Final Project for Introduction to Deep Learning Course at University of South Florida\n",
        "### Team Members: Jun Kim, Gerardo Wibmer Gonzalez, Paul-Ann Francis, Tahsun Rahman Khan\n",
        "\n",
        "## Overview\n",
        "This project aims to predict the closing prices of stock market indices using deep learning models built with Python and TensorFlow. We have developed a two-model approach to achieve this goal. The first model (Model 1) predicts the closing price using four input features (Open, High, Low, and Volume) for a specific date. The second model (Model 2) predicts the future values of each feature based on their respective historical data. By combining the predictions from these two models, we can estimate the closing price for any given day.\n",
        "\n",
        "Although our project focuses on predicting the S&P 500 index prices, the code can be easily modified to predict the prices of any stock.\n",
        "\n",
        "## Data\n",
        "The data used in this project consists of daily stock market index prices, including Open, High, Low, Close, and Volume. We have used the historical price data of the S&P 500 index, obtained from Yahoo Finance, to train, validate, and test our models.\n",
        "\n",
        "## Approach\n",
        "1. Preprocessing: The raw data is preprocessed to create a windowed dataset, which includes the necessary features and target variables.\n",
        "2. Model Training: We train separate instances of Model 2 for each feature (Open, High, Low, and Volume) using their historical data. This results in four different models that predict future values for each feature. Then, we train Model 1 using the combined features from Model 2's predictions.\n",
        "3. Hyperparameter Tuning: We used grid search to find the best hyperparameters for our models. The optimal hyperparameters and their corresponding performance metrics are stored in CSV files under the hyperparameters/ directory.\n",
        "4. Prediction: We input the predicted values of Open, High, Low, and Volume from Model 2 into Model 1 to predict the closing price for any given day.\n",
        "\n",
        "## Repository Structure\n",
        "- models/: Contains the saved deep learning models (Model 1 and Model 2 instances) for each feature.\n",
        "- hyperparameters/: Contains CSV files with the optimal hyperparameters and their corresponding performance metrics, obtained through grid search.\n",
        "- notebooks/: Contains Jupyter notebooks for data preprocessing, model training, and evaluation.\n",
        "- README.md: Provides an overview of the project, including the approach, data, and repository structure.\n",
        "\n",
        "## Dependencies\n",
        "- Python 3.8 or higher\n",
        "- datetime\n",
        "- matplotlib\n",
        "- NumPy\n",
        "- pandas\n",
        "- scikit-learn\n",
        "- TensorFlow\n",
        "- Keras\n",
        "- yfinance\n",
        "\n",
        "## License\n",
        "This project is licensed under the Apache License 2.0. See LICENSE file for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "PSIHfWL23fBi",
        "outputId": "2bc754fe-8594-4eb0-ac21-98906c684c62"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, LeakyReLU, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "import yfinance as yf"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_START_DATE = '1960-01-01'\n",
        "TRAIN_END_DATE = '2015-12-31'\n",
        "PREDICT_START_DATE = '2016-01-01'\n",
        "PREDICT_END_DATE = '2019-12-31'\n",
        "WINDOW_SIZE = 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download S&P 500 data from Yahoo Finance\n",
        "df = yf.download('^GSPC', start=TRAIN_START_DATE, end=PREDICT_END_DATE)\n",
        "\n",
        "df = df.reset_index()\n",
        "\n",
        "def str_to_datetime(s):\n",
        "  split = s.split('-')\n",
        "  year, month, day = int(split[0]), int(split[1]), int(split[2])\n",
        "  return datetime.datetime(year=year, month=month, day=day)\n",
        "\n",
        "df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
        "df['Date'] = df['Date'].apply(str_to_datetime)\n",
        "\n",
        "df.index = df.pop('Date')\n",
        "\n",
        "# Drop columns that are not needed\n",
        "df = df.drop(columns=['Adj Close'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into train and test sets\n",
        "X_train = df[:PREDICT_START_DATE]\n",
        "X_test = df[PREDICT_START_DATE:PREDICT_END_DATE]\n",
        "\n",
        "y_train = X_train.pop('Close')\n",
        "y_test = X_test.pop('Close')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def df_to_windowed_df(feature, df, window_size, start_date=PREDICT_START_DATE, end_date=PREDICT_END_DATE):\n",
        "    \"\"\"Converts a dataframe into a windowed dataframe and date list\"\"\"\n",
        "    df = df[start_date:end_date]\n",
        "    date_list = df.index.to_list()\n",
        "    feature_values = df[feature].to_numpy()\n",
        "    windowed_df = []\n",
        "    for i in range(len(feature_values) - window_size):\n",
        "        windowed_df.append(feature_values[i:i+window_size])\n",
        "    return np.array(windowed_df), date_list[window_size:]\n",
        "\n",
        "# Create windowed dataframes and date_list\n",
        "open_windowed_df, date_list = df_to_windowed_df('Open', df, window_size=WINDOW_SIZE, start_date=PREDICT_START_DATE, end_date=PREDICT_END_DATE)\n",
        "high_windowed_df, date_list = df_to_windowed_df('High', df, window_size=WINDOW_SIZE, start_date=PREDICT_START_DATE, end_date=PREDICT_END_DATE)\n",
        "low_windowed_df, date_list = df_to_windowed_df('Low', df, window_size=WINDOW_SIZE, start_date=PREDICT_START_DATE, end_date=PREDICT_END_DATE)\n",
        "volume_windowed_df, date_list = df_to_windowed_df('Volume', df, window_size=WINDOW_SIZE, start_date=PREDICT_START_DATE, end_date=PREDICT_END_DATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "to_combine_open_windowed_df, _ = df_to_windowed_df('Open', df, window_size=1, start_date=PREDICT_START_DATE, end_date=PREDICT_END_DATE)\n",
        "to_combine_high_windowed_df, _ = df_to_windowed_df('High', df, window_size=1, start_date=PREDICT_START_DATE, end_date=PREDICT_END_DATE)\n",
        "to_combine_low_windowed_df, _ = df_to_windowed_df('Low', df, window_size=1, start_date=PREDICT_START_DATE, end_date=PREDICT_END_DATE)\n",
        "to_combine_volume_windowed_df, _ = df_to_windowed_df('Volume', df, window_size=1, start_date=PREDICT_START_DATE, end_date=PREDICT_END_DATE)\n",
        "\n",
        "# Stack the windowed dataframes along the third axis\n",
        "stacked_windowed_df = np.stack([open_windowed_df, high_windowed_df, low_windowed_df, volume_windowed_df], axis=-1)\n",
        "\n",
        "# Reshape the stacked_windowed_df into the desired shape (975, 30, 4)\n",
        "combined_windowed_df = stacked_windowed_df.reshape((-1, WINDOW_SIZE, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def windowed_df_to_date_X_y(windowed_df, date_list):\n",
        "    \"\"\"Converts a windowed dataframe and date list into a date, X, and y dataframe\"\"\"\n",
        "    date_df = []\n",
        "    X_df = []\n",
        "    y_df = []\n",
        "    for i in range(len(windowed_df) - 1):  # Modify the range to exclude the last window\n",
        "        date_df.append(date_list[i + 1])   # Shift date by 1\n",
        "        X_df.append(windowed_df[i])\n",
        "        if windowed_df.ndim == 3:\n",
        "            y_df.append(windowed_df[i + 1][-1][-1])  # Shift the target y value by 1 (for 3D input)\n",
        "        else:\n",
        "            y_df.append(windowed_df[i + 1][-1])  # Shift the target y value by 1 (for 2D input)\n",
        "    return np.array(date_df), np.array(X_df), np.array(y_df)\n",
        "\n",
        "# Use the modified function with the date_list parameter\n",
        "date_df, X_open, y_open = windowed_df_to_date_X_y(open_windowed_df, date_list)\n",
        "_, X_high, y_high = windowed_df_to_date_X_y(high_windowed_df, date_list)\n",
        "_, X_low, y_low = windowed_df_to_date_X_y(low_windowed_df, date_list)\n",
        "_, X_volume, y_volume = windowed_df_to_date_X_y(volume_windowed_df, date_list)\n",
        "_, X_train_combined, _ = windowed_df_to_date_X_y(combined_windowed_df, date_list)\n",
        "y_train_combined = y_train[len(y_train) - len(X_train_combined):].to_numpy()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CreateModel:\n",
        "    def __init__(self, feature, dates, X_train, y_train, window_size, params=None):\n",
        "        self.feature = feature\n",
        "        self.dates = dates\n",
        "        self.X = X_train    \n",
        "        self.y = y_train\n",
        "        self.window_size = window_size\n",
        "        self.params = params\n",
        "\n",
        "        if self.params is None:\n",
        "            self.best_model = self.train_model(True)\n",
        "        else:\n",
        "            self.best_model = self.train_model(False)\n",
        "\n",
        "    def create_model(self, lstm_units=64, dense_units=32, learning_rate=0.001, lstm_activation='tanh', dense_activation='relu'):\n",
        "        model = Sequential([\n",
        "            Input((self.window_size, 1)),\n",
        "            LSTM(lstm_units, activation=lstm_activation),\n",
        "        ])\n",
        "\n",
        "        if dense_activation == 'leaky_relu':\n",
        "            model.add(Dense(dense_units))\n",
        "            model.add(LeakyReLU(alpha=0.3))\n",
        "            model.add(Dense(dense_units))\n",
        "            model.add(LeakyReLU(alpha=0.3))\n",
        "        else:\n",
        "            model.add(Dense(dense_units, activation=dense_activation))\n",
        "            model.add(Dense(dense_units, activation=dense_activation))\n",
        "            model.add(Dense(1))\n",
        "\n",
        "        model.compile(loss='mse',\n",
        "                    optimizer=Adam(learning_rate=learning_rate),\n",
        "                    metrics=['mean_absolute_error'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_model(self, grid_search):\n",
        "        # Check if GPU is available\n",
        "        if tf.config.list_physical_devices('GPU'):\n",
        "            print(\"Using GPU\")\n",
        "            # Set the device to GPU:0\n",
        "            with tf.device('/GPU:0'):\n",
        "                best_model = self.train_model_helper(grid_search)\n",
        "        else:\n",
        "            print(\"Using CPU\")\n",
        "            # If GPU is not available, perform grid search on CPU\n",
        "            best_model = self.train_model_helper(grid_search)\n",
        "\n",
        "        return best_model\n",
        "\n",
        "    def train_model_helper(self, grid_search):\n",
        "        # Wrap the create_model function with KerasRegressor\n",
        "        model = KerasRegressor(build_fn=self.create_model, verbose=1)\n",
        "\n",
        "        # Define the grid search parameters\n",
        "        param_grid = {\n",
        "            'lstm_units': [8, 16],\n",
        "            'dense_units': [4, 8],\n",
        "            'learning_rate': [0.0001, 0.0005],\n",
        "            'epochs': [30, 50],\n",
        "            'batch_size': [8, 16],\n",
        "            'lstm_activation': ['tanh', 'relu'],\n",
        "            'dense_activation': ['relu', 'elu']\n",
        "        }\n",
        "\n",
        "        # Create the GridSearchCV object\n",
        "        grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=6, cv=3, verbose=1)\n",
        "\n",
        "        # Split the data into training and validation\n",
        "        q_80 = int(len(self.dates) * .8)\n",
        "\n",
        "        self.dates_train, X_train, y_train = self.dates[:q_80], self.X[:q_80], self.y[:q_80]\n",
        "        self.dates_val, X_val, y_val = self.dates[q_80:], self.X[q_80:], self.y[q_80:]\n",
        "\n",
        "        # Flatten the training data for GridSearchCV\n",
        "        X_train_flat = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "        y_train_flat = y_train.flatten()\n",
        "        \n",
        "        if grid_search:\n",
        "            # Perform grid search\n",
        "            grid_result = grid.fit(X_train_flat, y_train_flat)\n",
        "\n",
        "            # Print the best hyperparameters\n",
        "            print(\"Best score: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "            # Create a model with the best hyperparameters\n",
        "            best_params = grid_result.best_params_\n",
        "\n",
        "            # Save the best hyperparameters\n",
        "            best_params_df = pd.DataFrame(best_params, index=[0])\n",
        "            best_params_df.to_csv(f'../hyperparameters/{self.feature}_best_params.csv', index=False)\n",
        "\n",
        "            best_epochs = best_params.pop('epochs')\n",
        "            best_batch_size = best_params.pop('batch_size')\n",
        "            best_model = self.create_model(**best_params)\n",
        "        else:\n",
        "            print(\"Using best hyperparameters from previous run\")\n",
        "            best_epochs = self.params.pop('epochs')\n",
        "            best_batch_size = self.params.pop('batch_size')\n",
        "            best_model = self.create_model(**self.params)\n",
        "\n",
        "        # Train the best model with the training data using default batch_size and epochs\n",
        "        best_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=best_epochs, batch_size=best_batch_size)\n",
        "\n",
        "        return best_model\n",
        "\n",
        "    def predict(self):\n",
        "        train_predictions = self.best_model.predict(self.X).flatten()\n",
        "        return train_predictions\n",
        "\n",
        "    def plot(self):\n",
        "        plt.figure(figsize=(20, 10))\n",
        "        plt.plot(self.dates, self.y, label='Actual')\n",
        "        plt.plot(self.dates, self.predict(), label='Predicted')\n",
        "        plt.legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Only run when you're want to grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(\"Creating Open Model\")\n",
        "# open_model = CreateModel('Open', df, START_DATE, END_DATE, n=3)\n",
        "# print(\"Creating High Model\")\n",
        "# high_model = CreateModel('High', df, START_DATE, END_DATE, n=3)\n",
        "# print(\"Creating Low Model\")\n",
        "# low_model = CreateModel('Low', df, START_DATE, END_DATE, n=3)\n",
        "# print(\"Creating Volume Model\")\n",
        "# volume_model = CreateModel('Volume', df, START_DATE, END_DATE, n=3)\n",
        "# print(\"Done\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Only Run if you want to train and save the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "open_params = pd.read_csv('../hyperparameters/Open_best_params.csv').to_dict(orient='records')[0]\n",
        "open_model = CreateModel('Open', date_df, X_open, y_open, WINDOW_SIZE, params=open_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "open_model.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "open_model.best_model.save('../models/open_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "high_params = pd.read_csv('../hyperparameters/High_best_params.csv').to_dict(orient='records')[0]\n",
        "high_model = CreateModel('High', date_df, X_high, y_high, WINDOW_SIZE, params=high_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "high_model.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "high_model.best_model.save('../models/high_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "low_params = pd.read_csv('../hyperparameters/Low_best_params.csv').to_dict(orient='records')[0]\n",
        "low_model = CreateModel('Low', date_df, X_low, y_low, WINDOW_SIZE, params=low_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "low_model.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "low_model.best_model.save('../models/low_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "volume_params = pd.read_csv('../hyperparameters/Volume_best_params.csv').to_dict(orient='records')[0]\n",
        "volume_model = CreateModel('Volume', date_df, X_volume, y_volume, WINDOW_SIZE, params=volume_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "volume_model.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "volume_model.best_model.save('../models/volume_model.h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Restore models\n",
        "You have to run this even when you're training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "open_model = tf.keras.models.load_model('../models/open_model.h5')\n",
        "high_model = tf.keras.models.load_model('../models/high_model.h5')\n",
        "low_model = tf.keras.models.load_model('../models/low_model.h5')\n",
        "volume_model = tf.keras.models.load_model('../models/volume_model.h5')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Restored Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the preidction and actual values\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.plot(date_df, y_open, label='Actual Open')\n",
        "plt.plot(date_df, open_model.predict(X_open), label='Predicted Open')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the preidction and actual values\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.plot(date_df, y_high, label='Actual High')\n",
        "plt.plot(date_df, high_model.predict(X_high), label='Predicted High')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the preidction and actual values\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.plot(date_df, y_low, label='Actual Low')\n",
        "plt.plot(date_df, low_model.predict(X_low), label='Predicted Low')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the preidction and actual values\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.plot(date_df, y_volume, label='Actual Volume')\n",
        "plt.plot(date_df, volume_model.predict(X_volume), label='Predicted Volume')\n",
        "plt.legend()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
